{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Backpropogation_2019MSBDA011.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+UjS+dOkGf+0IapzA8fFI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtgindia/Google-Colab-ML/blob/master/Backpropogation_2019MSBDA011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6LoXFYeaFs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a024QNkDaIUk",
        "colab_type": "text"
      },
      "source": [
        "Based on Ryan Harris's youtube video\n",
        "\n",
        "\n",
        "1.   Neural network tutorial: The back-propagation algorithm (Part 1)\n",
        "https://www.youtube.com/watch?v=aVId8KMsdUU\n",
        "2.   Neural network tutorial: The back-propagation algorithm (Part 2)\n",
        "https://www.youtube.com/watch?v=zpykfC4VnpM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we0en-bMVLzF",
        "colab_type": "code",
        "outputId": "3397b89a-76ce-4908-feb6-96716318f898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "\n",
        "\n",
        "class NN:\n",
        "  def __init__(self, NI, NH, NO):\n",
        "    # number of nodes in layers\n",
        "    self.ni = NI + 1 # +1 for bias\n",
        "    self.nh = NH\n",
        "    self.no = NO\n",
        "    \n",
        "    # initialize node-activations\n",
        "    self.ai, self.ah, self.ao = [],[], []\n",
        "    self.ai = [1.0]*self.ni\n",
        "    self.ah = [1.0]*self.nh\n",
        "    self.ao = [1.0]*self.no\n",
        "\n",
        "    # create node weight matrices\n",
        "    self.wi = makeMatrix (self.ni, self.nh)\n",
        "    self.wo = makeMatrix (self.nh, self.no)\n",
        "    # initialize node weights to random vals\n",
        "    randomizeMatrix ( self.wi, -0.2, 0.2 )\n",
        "    randomizeMatrix ( self.wo, -2.0, 2.0 )\n",
        "    # create last change in weights matrices for momentum\n",
        "    self.ci = makeMatrix (self.ni, self.nh)\n",
        "    self.co = makeMatrix (self.nh, self.no)\n",
        "    \n",
        "  def runNN (self, inputs):\n",
        "    if len(inputs) != self.ni-1:\n",
        "      print('incorrect number of inputs')\n",
        "    \n",
        "    for i in range(self.ni-1):\n",
        "      self.ai[i] = inputs[i]\n",
        "      \n",
        "    for j in range(self.nh):\n",
        "      sum = 0.0\n",
        "      for i in range(self.ni):\n",
        "        sum +=( self.ai[i] * self.wi[i][j] )\n",
        "      self.ah[j] = sigmoid (sum)\n",
        "    \n",
        "    for k in range(self.no):\n",
        "      sum = 0.0\n",
        "      for j in range(self.nh):        \n",
        "        sum +=( self.ah[j] * self.wo[j][k] )\n",
        "      self.ao[k] = sigmoid (sum)\n",
        "      \n",
        "    return self.ao\n",
        "      \n",
        "      \n",
        "  \n",
        "  def backPropagate (self, targets, N, M):\n",
        "  \n",
        "    \n",
        "    # calc output deltas\n",
        "    # we want to find the instantaneous rate of change of ( error with respect to weight from node j to node k)\n",
        "    # output_delta is defined as an attribute of each ouput node. It is not the final rate we need.\n",
        "    # To get the final rate we must multiply the delta by the activation of the hidden layer node in question.\n",
        "    # This multiplication is done according to the chain rule as we are taking the derivative of the activation function\n",
        "    # of the ouput node.\n",
        "    # dE/dw[j][k] = (t[k] - ao[k]) * s'( SUM( w[j][k]*ah[j] ) ) * ah[j]\n",
        "    output_deltas = [0.0] * self.no\n",
        "    for k in range(self.no):\n",
        "      error = targets[k] - self.ao[k]\n",
        "      output_deltas[k] =  error * dsigmoid(self.ao[k]) \n",
        "   \n",
        "    # update output weights\n",
        "    for j in range(self.nh):\n",
        "      for k in range(self.no):\n",
        "        # output_deltas[k] * self.ah[j] is the full derivative of dError/dweight[j][k]\n",
        "        change = output_deltas[k] * self.ah[j]\n",
        "        self.wo[j][k] += N*change + M*self.co[j][k]\n",
        "        self.co[j][k] = change\n",
        "\n",
        "    # calc hidden deltas\n",
        "    hidden_deltas = [0.0] * self.nh\n",
        "    for j in range(self.nh):\n",
        "      error = 0.0\n",
        "      for k in range(self.no):\n",
        "        error += output_deltas[k] * self.wo[j][k]\n",
        "      hidden_deltas[j] = error * dsigmoid(self.ah[j])\n",
        "    \n",
        "    #update input weights\n",
        "    for i in range (self.ni):\n",
        "      for j in range (self.nh):\n",
        "        change = hidden_deltas[j] * self.ai[i]\n",
        "        #print 'activation',self.ai[i],'synapse',i,j,'change',change\n",
        "        self.wi[i][j] += N*change + M*self.ci[i][j]\n",
        "        self.ci[i][j] = change\n",
        "        \n",
        "    # calc combined error\n",
        "    # 1/2 for differential convenience & **2 for modulus\n",
        "    error = 0.0\n",
        "    for k in range(len(targets)):\n",
        "      error = 0.5 * (targets[k]-self.ao[k])**2\n",
        "    return error\n",
        "        \n",
        "        \n",
        "  def weights(self):\n",
        "    print('Input weights:')\n",
        "    for i in range(self.ni):\n",
        "      print(self.wi[i])\n",
        "    print()\n",
        "    print('Output weights:')\n",
        "    for j in range(self.nh):\n",
        "      print(self.wo[j])\n",
        "    print('')\n",
        "  \n",
        "  def test(self, patterns):\n",
        "    for p in patterns:\n",
        "      inputs = p[0]\n",
        "      print('Inputs:', p[0], '-->', self.runNN(inputs), '\\tTarget', p[1])\n",
        "  \n",
        "  def train (self, patterns, max_iterations = 1000, N=0.5, M=0.1):\n",
        "    for i in range(max_iterations):\n",
        "      for p in patterns:\n",
        "        inputs = p[0]\n",
        "        targets = p[1]\n",
        "        self.runNN(inputs)\n",
        "        error = self.backPropagate(targets, N, M)\n",
        "      if i % 50 == 0:\n",
        "        print('Combined error', error)\n",
        "    self.test(patterns)\n",
        "    \n",
        "\n",
        "def sigmoid (x):\n",
        "  return math.tanh(x)\n",
        "  \n",
        "# the derivative of the sigmoid function in terms of output\n",
        "\n",
        "def dsigmoid (y):\n",
        "  return 1 - y**2\n",
        "\n",
        "def makeMatrix ( I, J, fill=0.0):\n",
        "  m = []\n",
        "  for i in range(I):\n",
        "    m.append([fill]*J)\n",
        "  return m\n",
        "  \n",
        "def randomizeMatrix ( matrix, a, b):\n",
        "  for i in range ( len (matrix) ):\n",
        "    for j in range ( len (matrix[0]) ):\n",
        "      matrix[i][j] = random.uniform(a,b)\n",
        "\n",
        "\n",
        "pat = [\n",
        "      [[0,0], [1]],\n",
        "      [[0,1], [1]],\n",
        "      [[1,0], [1]],\n",
        "      [[1,1], [0]]\n",
        "  ]\n",
        "myNN = NN ( 2, 2, 1)\n",
        "myNN.train(pat)\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Combined error 0.3282416825393011\n",
            "Combined error 0.04026088704589988\n",
            "Combined error 0.006817982584232518\n",
            "Combined error 0.0030376522370028736\n",
            "Combined error 0.0019249807962475173\n",
            "Combined error 0.0013987491113615258\n",
            "Combined error 0.0010941277258197176\n",
            "Combined error 0.0008961212842817046\n",
            "Combined error 0.0007573635761908027\n",
            "Combined error 0.0006549392541528011\n",
            "Combined error 0.0005764029446039553\n",
            "Combined error 0.0005142466075225312\n",
            "Combined error 0.0004638843830767625\n",
            "Combined error 0.0004223182528549592\n",
            "Combined error 0.0003873886557537991\n",
            "Combined error 0.0003576923865595473\n",
            "Combined error 0.0003320987058556986\n",
            "Combined error 0.000309857873263435\n",
            "Combined error 0.00029032498545469973\n",
            "Combined error 0.0002730654091203348\n",
            "Inputs: [0, 0] --> [0.999098632629462] \tTarget [1]\n",
            "Inputs: [0, 1] --> [0.9895572634568914] \tTarget [1]\n",
            "Inputs: [1, 0] --> [0.9895328567900159] \tTarget [1]\n",
            "Inputs: [1, 1] --> [0.01455278999812955] \tTarget [0]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}