{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron_2019MSBDA011.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaxEVvm0EaoLoWrYYU2y3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gtgindia/Google-Colab-ML/blob/master/Perceptron_2019MSBDA011.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxOKSd6hqA9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftVQc1VIsH8P",
        "colab_type": "text"
      },
      "source": [
        "# Perceptron Functions\n",
        "\n",
        "*  **__init__**:  In order to construct our perceptron, we need to know how many inputs there are to create our weight vector. The reason we add one to the input size is to include the bias in the weight vector.\n",
        "*   **activation_fn**:  Weâ€™ll also need to implement our activation function. We can simply return 1 if the input is greater than or equal to 0 and 0 otherwise.\n",
        "*   **predict**:  We need a function to run an input through the perceptron and return an output.We add the bias into the input vector. Then we can simply compute the inner product and apply the activation function.\n",
        "*    **fit**:  Then we can create our prediction, compute our error, and perform our update rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UFeKTH0HsTD5",
        "colab": {}
      },
      "source": [
        "class Perceptron(object):\n",
        "    def __init__(self, input_size, lr=1, epochs=100):\n",
        "        self.W = np.zeros(input_size+1)\n",
        "        # add one for bias\n",
        "        self.epochs = epochs\n",
        "        self.lr = lr\n",
        "    \n",
        "    def activation_fn(self, x):\n",
        "        #return (x >= 0).astype(np.float32)\n",
        "        return 1 if x >= 0 else 0\n",
        " \n",
        "    def predict(self, x):\n",
        "        z = self.W.T.dot(x)\n",
        "        a = self.activation_fn(z)\n",
        "        return a\n",
        " \n",
        "    def fit(self, X, d):\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(d.shape[0]):\n",
        "                x = np.insert(X[i], 0, 1)\n",
        "                y = self.predict(x)\n",
        "                e = d[i] - y\n",
        "                self.W = self.W + self.lr * e * x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hngq5XDp2v-",
        "colab_type": "code",
        "outputId": "c8e5335b-f70e-4ded-9167-38ea48bdc5c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X = np.array([\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "    ])\n",
        "d = np.array([0, 0, 0, 1])\n",
        " \n",
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.fit(X, d)\n",
        "print(perceptron.W)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-3.  2.  1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jqP5uf2uUby",
        "colab_type": "text"
      },
      "source": [
        " This means that the bias is -3 and the weights are 2 and 1 for x_1 and x_2, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo8TuMEtuM1s",
        "colab_type": "text"
      },
      "source": [
        " If both inputs are 0, then the pre-activation will be -3+0x2+0x1 = -3. When applying our activation function, we get 0, which is exactly 0 AND 0!"
      ]
    }
  ]
}